---
title: "TREEregression"
author: "Yuxuan Li"
date: "9/3/2019"
output:
  prettydoc::html_pretty:
    theme: leonids
    highlight: github
    math: katex
---

```{r setup}
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE)

library(dplyr)
library(randomForest)
library(tree)

load("bnb_data.Rdata")

# define function
  rmse = function(actual, predicted) {
    sqrt(mean((actual - predicted) ^ 2))
  }

# data process
  
  # quant
  quant <- dplyr::select(bnb.data, X, log_price, latitude, longitude, 
                          accommodates, beds, bedrooms, bathrooms, number_of_amenities,
                          review_scores_rating, number_of_reviews, host_response_rate) %>%
    na.omit()
  
  train.quant <- filter(quant, X %in% train.data$X) %>% dplyr::select(-X)
  test.quant <- filter(quant, X %in% test.data$X) %>% dplyr::select(-X)
  
  # whole 
  train.whole <- dplyr::select(train.data, -amenities, 
                               -host_since,
                               -is_Private,-is_Entire,
                               -neighbourhood, -zipcode, -X) %>% na.omit()
  test.whole <- dplyr::select(test.data, -amenities, 
                              -host_since,
                              -is_Private,-is_Entire,
                              -neighbourhood, -zipcode, -X) %>%  na.omit()
  

```


```{r random quant, eval=FALSE}

### Random Forests: 
set.seed(1)

# Build random forest with mtry = 6
rf.bnb <- randomForest(log_price~., data = train.quant, mtry = 6, importance = TRUE)
yhat.rf <- predict(rf.bnb,newdata=test.quant)
mean((yhat.rf-test.quant$log_price)^2)   # Test MSE
sqrt(mean((yhat.rf-test.quant$log_price)^2))   # sqrt test MSE

importance(rf.bnb)
varImpPlot(rf.bnb)

```




```{r random whole, eval=FALSE}
### Random Forests: Just change mtry argument!
# Default options in randomForest(): for regression tree p/3, for classification tree sqrt(p)
set.seed(1)
# Build random forest with mtry = 6
rf.bnb <- randomForest(log_price~., data = train.whole, mtry = 6, importance = TRUE)
# use importance = T to see the importance of each variable 
yhat.rf <- predict(rf.bnb,newdata=test.whole)
mean((yhat.rf-test.whole$log_price)^2)   # Test MSE
sqrt(mean((yhat.rf-test.whole$log_price)^2))   # sqrt test MSE

# Do you see any improvement over single reg. tree and bagging? 
# What might be the reason?
# Why are trees correlated? becuase our bootstrapping sample contains overlapping obs, sort of correlated. 

importance(rf.bnb)
varImpPlot(rf.bnb)
# What are the two importance measures?
# left--accuracy, right -- purity
# How to interpret these plots?

```


```{r cv tree}
train.whole <- select(train.whole, -property_type)
test.whole  <- select(test.whole,  -property_type)
# do an innitial tree
bnb_tree = tree(log_price ~ ., data = train.whole)
summary(bnb_tree)

# plot it
plot(bnb_tree)
text(bnb_tree, pretty = 0)
title(main = "Unpruned Regression Tree")

# using cv to prune
set.seed(1)
bnb_tree_cv = cv.tree(bnb_tree)
plot(bnb_tree_cv$size, sqrt(bnb_tree_cv$dev / nrow(train.whole)), type = "b",
     xlab = "Tree Size", ylab = "CV-RMSE")

# apply best = 8
bnb_tree_prune = prune.tree(bnb_tree, best = 8)
summary(bnb_tree_prune)

# plot it
par(mfrow = c(1,2))
plot(bnb_tree)
text(bnb_tree, pretty = 0)
title(main = "Unpruned Regression Tree")
plot(bnb_tree_prune)
text(bnb_tree_prune, pretty = 0)
title(main = "Pruned Regression Tree")

## Evaluation
# training RMSE
bnb_prune_trn_pred = predict(bnb_tree_prune, newdata = train.whole)
rmse(bnb_prune_trn_pred, train.whole$log_price)
# test RMSE
bnb_prune_tst_pred = predict(bnb_tree_prune, newdata = test.whole)
rmse(bnb_prune_tst_pred, test.whole$log_price)

dev.off()
plot(bnb_prune_tst_pred, test.whole$log_price, xlab = "Predicted", ylab = "Actual")
abline(0, 1)
```

```{r rpart tree}
# https://daviddalpiaz.github.io/r4sl/trees.html#regression-trees

library(rpart)
set.seed(1)
# Fit a decision tree using rpart
# Note: when you fit a tree using rpart, the fitting routine automatically
# performs 10-fold CV and stores the errors for later use 
# (such as for pruning the tree)

# fit a tree using rpart
bnb_rpart = rpart(log_price ~ ., data = train.whole, control = c(cp=0.0002))

# plot the cv error curve for the tree
# rpart tries different cost-complexities by default
# also stores cv results
plotcp(bnb_rpart, upper = "size")


# find best value of cp
v = which.min(bnb_rpart$cptable[,"xerror"])
min_cp = bnb_rpart$cptable[v,"CP"]
best_split = bnb_rpart$cptable[v,"nsplit"]
min_cp;best_split


plotcp(bnb_rpart, upper = "splits")
abline(v = v, col = "red")
abline(v = 7, col = "blue")

# prunce tree using best cp
bnb_rpart_prunel = prune(bnb_rpart, cp = min_cp)
bnb_rpart_prunes = prune(bnb_rpart, cp = 0.01)

# nicer plots
library(rpart.plot)
prp(bnb_rpart_prunel)
rpart.plot(bnb_rpart_prunel)
prp(bnb_rpart_prunes)
rpart.plot(bnb_rpart_prunes)

## Evaluation
# training RMSE large tree
bnb_rpart_trn_predl = predict(bnb_rpart_prunel, newdata = train.whole)
rmse_train_large = rmse(bnb_rpart_trn_predl, train.whole$log_price)
# test RMSE large tree
bnb_rpart_tst_predl = predict(bnb_rpart_prunel, newdata = test.whole)
rmse_test_large = rmse(bnb_rpart_tst_predl, test.whole$log_price)

# training RMSE small tree
bnb_rpart_trn_preds = predict(bnb_rpart_prunes, newdata = train.whole)
rmse_train_small = rmse(bnb_rpart_trn_preds, train.whole$log_price)
# test RMSE large tree
bnb_rpart_tst_preds = predict(bnb_rpart_prunes, newdata = test.whole)
rmse_test_small = rmse(bnb_rpart_tst_preds, test.whole$log_price)

t_rmse <- cbind(best=c(rmse_train_large, rmse_test_large),
                feas=c(rmse_train_small, rmse_test_small))
knitr::kable(t_rmse, escape = FALSE, booktabs = TRUE)

plot(test.whole$log_price, bnb_rpart_tst_predl,
     xlab = "Actual log price", ylab = "Predict log price", col = "lightblue")
abline(0,1)
```